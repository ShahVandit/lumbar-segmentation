{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V5E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# !pip install google-colab\n",
        "!pip install opencv-python\n",
        "!pip install -U albumentations\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjLZGBHDJj1q",
        "outputId": "c95f73fa-8297-4b4a-8fce-fb61853fc475"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencv-python\n",
            "  Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n",
            "Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opencv-python\n",
            "Successfully installed opencv-python-4.12.0.88\n",
            "Collecting albumentations\n",
            "  Downloading albumentations-2.0.8-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from albumentations) (1.16.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.11.7)\n",
            "Collecting albucore==0.0.24 (from albumentations)\n",
            "  Downloading albucore-0.0.24-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting opencv-python-headless>=4.9.0.80 (from albumentations)\n",
            "  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
            "Collecting stringzilla>=3.10.4 (from albucore==0.0.24->albumentations)\n",
            "  Downloading stringzilla-3.12.6-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting simsimd>=5.9.2 (from albucore==0.0.24->albumentations)\n",
            "  Downloading simsimd-6.5.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.5/70.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.1)\n",
            "Downloading albumentations-2.0.8-py3-none-any.whl (369 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m369.4/369.4 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading albucore-0.0.24-py3-none-any.whl (15 kB)\n",
            "Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (54.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading simsimd-6.5.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stringzilla-3.12.6-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (308 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.4/308.4 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: stringzilla, simsimd, opencv-python-headless, albucore, albumentations\n",
            "Successfully installed albucore-0.0.24 albumentations-2.0.8 opencv-python-headless-4.12.0.88 simsimd-6.5.1 stringzilla-3.12.6\n",
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/mri_data_png.zip\"   # adjust path if different\n",
        "extract_path = \"/content/mri_data_png\"\n",
        "\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Extracted to:\", extract_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TThf10XpKORZ",
        "outputId": "aad8a5dd-36fc-4040-bd07-bc8e0cad0943"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted to: /content/mri_data_png\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTLU5qE8JBtQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, down=True, act=\"relu\", use_dropout=False, norm=\"instance\"):\n",
        "        super(Block, self).__init__()\n",
        "\n",
        "        conv = (\n",
        "            nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False, padding_mode=\"reflect\")\n",
        "            if down else\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False)\n",
        "        )\n",
        "\n",
        "        # Choose normalization (default: InstanceNorm for small-batch stability)\n",
        "        if norm == \"instance\":\n",
        "            norm_layer = nn.InstanceNorm2d(out_channels, affine=True)\n",
        "        elif norm == \"batch\":\n",
        "            norm_layer = nn.BatchNorm2d(out_channels)\n",
        "        elif norm is None:\n",
        "            norm_layer = nn.Identity()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown norm='{norm}'. Use 'instance', 'batch', or None.\")\n",
        "\n",
        "        act_layer = nn.ReLU(inplace=True) if act == \"relu\" else nn.LeakyReLU(0.2, inplace=True)\n",
        "\n",
        "        self.conv = nn.Sequential(conv, norm_layer, act_layer)\n",
        "        self.use_dropout = use_dropout\n",
        "        self.dropout = nn.Dropout2d(0.5)  # keep Dropout (pix2pix uses it in some up blocks)\n",
        "\n",
        "        self.down = down  # (kept for clarity; not used inside forward)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return self.dropout(x) if self.use_dropout else x\n",
        "\n",
        "\n",
        "# your Block is the same one you just finalized\n",
        "# class Block(...):  # as you defined above\n",
        "#     ...\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# uses your finalized Block (InstanceNorm + Dropout2d etc.)\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_ch=1, out_ch=6, features=64):\n",
        "        super().__init__()\n",
        "        # Encoder: 256->128->64->32->16->8->4->2  (initial_down + 6 downs)\n",
        "        self.initial_down = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, features, 4, 2, 1, padding_mode=\"reflect\"),  # 256 -> 128\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "        self.down1 = Block(features,       features * 2, down=True, act=\"leaky\")   # 128 -> 64\n",
        "        self.down2 = Block(features * 2,   features * 4, down=True, act=\"leaky\")   # 64  -> 32\n",
        "        self.down3 = Block(features * 4,   features * 8, down=True, act=\"leaky\")   # 32  -> 16\n",
        "        self.down4 = Block(features * 8,   features * 8, down=True, act=\"leaky\")   # 16  -> 8\n",
        "        self.down5 = Block(features * 8,   features * 8, down=True, act=\"leaky\")   # 8   -> 4\n",
        "        self.down6 = Block(features * 8,   features * 8, down=True, act=\"leaky\")   # 4   -> 2\n",
        "\n",
        "        # Bottleneck: 2x2 -> 1x1\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Conv2d(features * 8, features * 8, 4, 2, 1),  # 2 -> 1\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        # Decoder: 1->2(cat d6)->4(cat d5)->8(cat d4)->16(cat d3)->32(cat d2)->64(cat d1)->128(cat d0)->256\n",
        "        self.up1 = Block(features * 8,       features * 8, down=False, act=\"relu\", use_dropout=True)        # 1 -> 2\n",
        "        self.up2 = Block(features * 8 * 2,   features * 8, down=False, act=\"relu\", use_dropout=True)        # 2 -> 4\n",
        "        self.up3 = Block(features * 8 * 2,   features * 8, down=False, act=\"relu\", use_dropout=True)        # 4 -> 8\n",
        "        self.up4 = Block(features * 8 * 2,   features * 8, down=False, act=\"relu\")                          # 8 -> 16\n",
        "        self.up5 = Block(features * 8 * 2,   features * 4, down=False, act=\"relu\")                          # 16 -> 32\n",
        "        self.up6 = Block(features * 4 * 2,   features * 2, down=False, act=\"relu\")                          # 32 -> 64\n",
        "        self.up7 = Block(features * 2 * 2,   features,     down=False, act=\"relu\")                          # 64 -> 128\n",
        "\n",
        "        # Final: 128 -> 256, logits for 6 classes (NO Tanh)\n",
        "        self.final_up = nn.ConvTranspose2d(features * 2, out_ch, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        d1 = self.initial_down(x)   # 128\n",
        "        d2 = self.down1(d1)         # 64\n",
        "        d3 = self.down2(d2)         # 32\n",
        "        d4 = self.down3(d3)         # 16\n",
        "        d5 = self.down4(d4)         # 8\n",
        "        d6 = self.down5(d5)         # 4\n",
        "        d7 = self.down6(d6)         # 2\n",
        "\n",
        "        b  = self.bottleneck(d7)    # 1\n",
        "\n",
        "        u1 = self.up1(b)                            # 2\n",
        "        u2 = self.up2(torch.cat([u1, d7], 1))       # 4\n",
        "        u3 = self.up3(torch.cat([u2, d6], 1))       # 8\n",
        "        u4 = self.up4(torch.cat([u3, d5], 1))       # 16\n",
        "        u5 = self.up5(torch.cat([u4, d4], 1))       # 32\n",
        "        u6 = self.up6(torch.cat([u5, d3], 1))       # 64\n",
        "        u7 = self.up7(torch.cat([u6, d2], 1))       # 128\n",
        "\n",
        "        logits = self.final_up(torch.cat([u7, d1], 1))  # 256\n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "G = Generator(in_ch=1, out_ch=6, features=64)\n",
        "\n",
        "# Dummy input: batch of 2 grayscale PNG slices (N=2, C=1, H=256, W=256)\n",
        "x = torch.randn(2, 1, 256, 256)\n",
        "\n",
        "# Forward pass\n",
        "y = G(x)\n",
        "\n",
        "print(\"Input shape :\", x.shape)\n",
        "print(\"Output shape:\", y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-v2fxkcqJGZR",
        "outputId": "5a8eaef8-5db4-45f9-e0a3-ec4df1980f8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape : torch.Size([2, 1, 256, 256])\n",
            "Output shape: torch.Size([2, 6, 256, 256])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CNNBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 4, stride, 1, bias=False, padding_mode=\"reflect\"),\n",
        "            nn.InstanceNorm2d(out_channels, affine=True),  # swapped from BatchNorm2d\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    \"\"\"\n",
        "    PatchGAN discriminator for MRI->mask at 256x256.\n",
        "\n",
        "    Expects either:\n",
        "      - x: (N, in_ch_x, H, W)\n",
        "      - y: (N, in_ch_y, H, W)   # one-hot or softmax probs\n",
        "    OR\n",
        "      - x: (N, in_ch_x, H, W)\n",
        "      - y: (N, H, W)            # integer class indices in [0..in_ch_y-1]\n",
        "\n",
        "    Returns: (N, 1, h, w) patch scores.\n",
        "    \"\"\"\n",
        "    def __init__(self, in_ch_x=1, in_ch_y=6, features=(64, 128, 256, 512)):\n",
        "        super().__init__()\n",
        "        self.in_ch_x = in_ch_x\n",
        "        self.in_ch_y = in_ch_y\n",
        "\n",
        "        in_pair = in_ch_x + in_ch_y\n",
        "\n",
        "        # First layer: no norm (pix2pix convention)\n",
        "        self.initial = nn.Sequential(\n",
        "            nn.Conv2d(in_pair, features[0], kernel_size=4, stride=2, padding=1, padding_mode=\"reflect\"),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "\n",
        "        blocks = []\n",
        "        in_c = features[0]\n",
        "        for f in features[1:]:\n",
        "            stride = 1 if f == features[-1] else 2\n",
        "            blocks.append(nn.Sequential(\n",
        "                nn.Conv2d(in_c, f, kernel_size=4, stride=stride, padding=1, bias=False, padding_mode=\"reflect\"),\n",
        "                nn.InstanceNorm2d(f, affine=True),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "            ))\n",
        "            in_c = f\n",
        "\n",
        "        # Final 1-channel conv -> patch score map\n",
        "        blocks.append(nn.Conv2d(in_c, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"))\n",
        "\n",
        "        self.model = nn.Sequential(*blocks)\n",
        "\n",
        "    def _ensure_mask_channels(self, y):\n",
        "        \"\"\"\n",
        "        Convert (N,H,W) index mask to (N,C,H,W) one-hot if needed.\n",
        "        If y already has shape (N,C,H,W), return as-is.\n",
        "        \"\"\"\n",
        "        if y.dim() == 3:\n",
        "            # y is class indices\n",
        "            if not (y.dtype == torch.long or y.dtype == torch.int64):\n",
        "                y = y.long()\n",
        "            y = F.one_hot(y, num_classes=self.in_ch_y).permute(0, 3, 1, 2).float()\n",
        "        elif y.dim() == 4:\n",
        "            # y is (N,C,H,W) already\n",
        "            # ensure channel count matches config\n",
        "            if y.size(1) != self.in_ch_y:\n",
        "                raise ValueError(f\"Expected mask with {self.in_ch_y} channels, got {y.size(1)}.\")\n",
        "            # if it's probs that's fine; if it's one-hot int, cast to float\n",
        "            if not y.is_floating_point():\n",
        "                y = y.float()\n",
        "        else:\n",
        "            raise ValueError(f\"Mask y must be (N,H,W) or (N,C,H,W), got shape {tuple(y.shape)}.\")\n",
        "        return y\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        \"\"\"\n",
        "        x: (N, in_ch_x, H, W)\n",
        "        y: (N, in_ch_y, H, W)  or  (N, H, W) indices\n",
        "        \"\"\"\n",
        "        y = self._ensure_mask_channels(y)\n",
        "        pair = torch.cat([x, y], dim=1)        # (N, in_ch_x+in_ch_y, H, W)\n",
        "        h = self.initial(pair)\n",
        "        out = self.model(h)\n",
        "        return out\n",
        "# quick sanity test for your shapes\n",
        "if __name__ == \"__main__\":\n",
        "    N, H, W = 2, 256, 256\n",
        "    D = Discriminator(in_ch_x=1, in_ch_y=6)\n",
        "\n",
        "    x = torch.randn(N, 1, H, W)               # grayscale\n",
        "    y_idx = torch.randint(0, 6, (N, H, W))    # indices\n",
        "    out_real = D(x, y_idx)\n",
        "    print(\"D(x, y_idx) ->\", out_real.shape)\n",
        "\n",
        "    y_probs = torch.softmax(torch.randn(N, 6, H, W), dim=1)\n",
        "    out_fake = D(x, y_probs)\n",
        "    print(\"D(x, y_probs) ->\", out_fake.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omd0AX2oJIPH",
        "outputId": "e86f54b2-a2cb-4711-b3d0-0d94126470f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "D(x, y_idx) -> torch.Size([2, 1, 30, 30])\n",
            "D(x, y_probs) -> torch.Size([2, 1, 30, 30])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, random, cv2, numpy as np, torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "# ---------- discover & split ----------\n",
        "def collect_ids(root=\"mri_data\", img_dir=\"images\", mask_dir=\"masks\"):\n",
        "    imgs  = {os.path.splitext(os.path.basename(p))[0]: p\n",
        "             for p in glob.glob(os.path.join(root, img_dir, \"*.png\"))}\n",
        "    masks = {os.path.splitext(os.path.basename(p))[0]: p\n",
        "             for p in glob.glob(os.path.join(root, mask_dir, \"*.png\"))}\n",
        "    ids = sorted(list(set(imgs.keys()) & set(masks.keys())))\n",
        "    if not ids:\n",
        "        raise RuntimeError(\"No matching image/mask basenames found.\")\n",
        "    return ids, imgs, masks\n",
        "\n",
        "def split_ids(ids, val_ratio=0.1, seed=42):\n",
        "    random.Random(seed).shuffle(ids)\n",
        "    n_val = max(1, int(len(ids) * val_ratio))\n",
        "    return ids[n_val:], ids[:n_val]  # train_ids, val_ids\n",
        "\n",
        "# ---------- dataset ----------\n",
        "class SpinePNG(Dataset):\n",
        "    def __init__(self, ids, img_map, mask_map):\n",
        "        self.ids = ids\n",
        "        self.img_map = img_map\n",
        "        self.mask_map = mask_map\n",
        "        self.tf = A.Compose([\n",
        "            A.Resize(256, 256),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.Normalize(mean=(0.5,), std=(0.5,)),  # assumes image scaled to [0,1]\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "\n",
        "    def __len__(self): return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        bid = self.ids[i]\n",
        "        img = cv2.imread(self.img_map[bid], cv2.IMREAD_UNCHANGED)  # 8/16-bit ok\n",
        "        msk = cv2.imread(self.mask_map[bid], cv2.IMREAD_UNCHANGED) # uint8 indices 0..5\n",
        "\n",
        "        # ensure grayscale\n",
        "        if img.ndim == 3:  # BGR -> gray\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        # scale image to [0,1]\n",
        "        if img.dtype == np.uint16:\n",
        "            img = img.astype(np.float32) / 65535.0\n",
        "        else:\n",
        "            img = img.astype(np.float32) / 255.0\n",
        "\n",
        "        # albumentations expects HxW\n",
        "        aug = self.tf(image=img, mask=msk.astype(np.int64))\n",
        "        x = aug[\"image\"]            # (1,256,256) because ToTensorV2 adds C for gray\n",
        "        if x.ndim == 2:             # safety: add channel if needed\n",
        "            x = x.unsqueeze(0)\n",
        "        y = aug[\"mask\"].long()      # (256,256) class indices\n",
        "        return x, y\n",
        "\n",
        "# ---------- build loaders ----------\n",
        "def make_loaders(root=\"mri_data\", batch_size=4, val_ratio=0.1, num_workers=4, seed=42):\n",
        "    ids, img_map, mask_map = collect_ids(root)\n",
        "    train_ids, val_ids = split_ids(ids, val_ratio=val_ratio, seed=seed)\n",
        "    train_ds = SpinePNG(train_ids, img_map, mask_map)\n",
        "    val_ds   = SpinePNG(val_ids,   img_map, mask_map)\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
        "                              num_workers=num_workers, pin_memory=True)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False,\n",
        "                              num_workers=max(1, num_workers//2), pin_memory=True)\n",
        "    return train_loader, val_loader\n",
        "\n",
        "train_loader, val_loader = make_loaders(root=\"/content/mri_data_png/data\", batch_size=4, val_ratio=0.1)\n",
        "\n",
        "for xb, yb in train_loader:\n",
        "    print(xb.shape, yb.shape)  # expect (N,1,256,256) and (N,256,256)\n",
        "    break\n",
        "print(len(train_loader.dataset))  # should be ~231\n",
        "print(len(val_loader.dataset))    # should be ~26"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rQwjTodJKEn",
        "outputId": "b4cc0fcf-5502-43d0-e79b-bd16b09f8822"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([4, 1, 256, 256]) torch.Size([4, 256, 256])\n",
            "3182\n",
            "353\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "NUM_CLASSES = 20\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "ce  = nn.CrossEntropyLoss()\n",
        "\n",
        "def dice_loss_from_logits(logits, target_idx, eps=1e-6):\n",
        "    p = F.softmax(logits, dim=1)                                   # (N,20,H,W)\n",
        "    t = F.one_hot(target_idx, NUM_CLASSES).permute(0,3,1,2).float()# (N,20,H,W)\n",
        "    num = 2 * (p * t).sum((0,2,3))\n",
        "    den = (p*p).sum((0,2,3)) + (t*t).sum((0,2,3)) + eps\n",
        "    return 1 - (num/den).mean()\n",
        "\n",
        "def train(gen, disc, train_loader, val_loader, device=\"cuda\", epochs=20, lr=2e-4, lambda_seg=20.0,\n",
        "          save_each_epoch=False):\n",
        "    gen.to(device); disc.to(device)\n",
        "\n",
        "    opt_g = torch.optim.Adam(gen.parameters(),  lr=lr, betas=(0.5, 0.999))\n",
        "    opt_d = torch.optim.Adam(disc.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "    amp_enabled = device.startswith(\"cuda\") and torch.cuda.is_available()\n",
        "    scaler_g = torch.cuda.amp.GradScaler(enabled=amp_enabled)\n",
        "    scaler_d = torch.cuda.amp.GradScaler(enabled=amp_enabled)\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        gen.train(); disc.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"epoch {epoch}/{epochs}\", ncols=100, leave=False)\n",
        "\n",
        "        for x, y_idx in pbar:\n",
        "            x = x.to(device, non_blocking=True)\n",
        "            y_idx = y_idx.to(device, non_blocking=True)\n",
        "\n",
        "            # ---- D step ----\n",
        "            with torch.cuda.amp.autocast(enabled=amp_enabled):\n",
        "                with torch.no_grad():\n",
        "                    logits_fake = gen(x)\n",
        "                    probs_fake  = F.softmax(logits_fake, dim=1)       # (N,20,H,W)\n",
        "                y_real = F.one_hot(y_idx, NUM_CLASSES).permute(0,3,1,2).float()\n",
        "                d_real = disc(x, y_real)\n",
        "                d_fake = disc(x, probs_fake.detach())\n",
        "                loss_d = bce(d_real, torch.ones_like(d_real)) + bce(d_fake, torch.zeros_like(d_fake))\n",
        "\n",
        "            opt_d.zero_grad(set_to_none=True)\n",
        "            scaler_d.scale(loss_d).backward()\n",
        "            scaler_d.step(opt_d); scaler_d.update()\n",
        "\n",
        "            # ---- G step ----\n",
        "            with torch.cuda.amp.autocast(enabled=amp_enabled):\n",
        "                logits = gen(x)\n",
        "                probs  = F.softmax(logits, dim=1)\n",
        "                d_fake_for_g = disc(x, probs)\n",
        "                gan_loss = bce(d_fake_for_g, torch.ones_like(d_fake_for_g))\n",
        "                seg_loss = ce(logits, y_idx) + dice_loss_from_logits(logits, y_idx)\n",
        "                loss_g = gan_loss + lambda_seg * seg_loss\n",
        "\n",
        "            opt_g.zero_grad(set_to_none=True)\n",
        "            scaler_g.scale(loss_g).backward()\n",
        "            scaler_g.step(opt_g); scaler_g.update()\n",
        "\n",
        "            pbar.set_postfix(D=f\"{loss_d.item():.3f}\",\n",
        "                              G=f\"{loss_g.item():.3f}\",\n",
        "                              CE=f\"{ce(logits, y_idx).item():.3f}\")\n",
        "\n",
        "        # ---- validation: CE + Dice ----\n",
        "        gen.eval()\n",
        "        ce_sum, dice_sum, n = 0.0, 0.0, 0\n",
        "        with torch.inference_mode():\n",
        "            for x, y_idx in val_loader:\n",
        "                x = x.to(device); y_idx = y_idx.to(device)\n",
        "                logits = gen(x)\n",
        "                ce_sum   += ce(logits, y_idx).item()\n",
        "                dice_sum += (1.0 - dice_loss_from_logits(logits, y_idx).item())\n",
        "                n += 1\n",
        "\n",
        "        avg_ce   = ce_sum / max(n, 1)\n",
        "        avg_dice = dice_sum / max(n, 1)\n",
        "        print(f\"[epoch {epoch}] val CE: {avg_ce:.3f} | val Dice: {avg_dice:.3f}\")\n",
        "\n",
        "        # optional: save once per epoch\n",
        "        if save_each_epoch:\n",
        "            torch.save(gen.state_dict(), f\"gen_epoch{epoch}.pth\")\n",
        "\n",
        "    # return AFTER all epochs\n",
        "    return gen\n"
      ],
      "metadata": {
        "id": "7UKeUf2gKwLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen  = Generator(in_ch=1, out_ch=NUM_CLASSES, features=64)\n",
        "disc = Discriminator(in_ch_x=1, in_ch_y=NUM_CLASSES)\n",
        "\n",
        "model = train(gen, disc, train_loader, val_loader, device=\"cpu\", epochs=20, lr=2e-4, lambda_seg=20.0)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgJKrJJSM_PA",
        "outputId": "b5f4907e-656f-4375-cde3-3488c6bc22a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3493265627.py:25: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler_g = torch.cuda.amp.GradScaler(enabled=amp_enabled)\n",
            "/tmp/ipython-input-3493265627.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler_d = torch.cuda.amp.GradScaler(enabled=amp_enabled)\n",
            "epoch 1/20:   0%|                                                           | 0/796 [00:00<?, ?it/s]/tmp/ipython-input-3493265627.py:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=amp_enabled):\n",
            "/tmp/ipython-input-3493265627.py:51: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=amp_enabled):\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 1] val CE: 0.213 | val Dice: 0.468\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 2] val CE: 0.181 | val Dice: 0.588\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 3] val CE: 0.134 | val Dice: 0.668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[epoch 4] val CE: 0.199 | val Dice: 0.666\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "epoch 5/20:  64%|████████████▊       | 509/796 [10:43<05:58,  1.25s/it, CE=0.056, D=0.044, G=12.732]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, csv\n",
        "import numpy as np\n",
        "import cv2\n",
        "from collections import Counter\n",
        "\n",
        "MASK_DIR = \"/content/mri_data_png/data/masks\"\n",
        "NUM_CLASSES = 6            # <-- set your current class count\n",
        "\n",
        "# --- aggregate stats ---\n",
        "global_counts = Counter()\n",
        "bad_files = []            # files containing labels >= NUM_CLASSES or < 0\n",
        "rgb_like = []             # files that load as 3-channel (likely color masks)\n",
        "per_file_stats = []       # (path, min, max, uniques_truncated)\n",
        "\n",
        "mask_paths = sorted(glob.glob(os.path.join(MASK_DIR, \"*.png\")))\n",
        "if not mask_paths:\n",
        "    raise RuntimeError(f\"No PNG masks found under {MASK_DIR}\")\n",
        "\n",
        "for p in mask_paths:\n",
        "    m = cv2.imread(p, cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "    if m is None:\n",
        "        print(f\"[WARN] Could not read: {p}\")\n",
        "        continue\n",
        "\n",
        "    # If mask is accidentally RGB, keep note; we’ll take single channel for auditing\n",
        "    if m.ndim == 3:\n",
        "        rgb_like.append(p)\n",
        "        m = m[..., 0]  # take first channel just to inspect values\n",
        "\n",
        "    # ensure integer type\n",
        "    if not np.issubdtype(m.dtype, np.integer):\n",
        "        # round if float; then cast\n",
        "        m = np.rint(m).astype(np.int64)\n",
        "    else:\n",
        "        m = m.astype(np.int64)\n",
        "\n",
        "    # collect unique values for this file\n",
        "    u, c = np.unique(m, return_counts=True)\n",
        "    global_counts.update(dict(zip(u.tolist(), c.tolist())))\n",
        "\n",
        "    # min/max and check bounds\n",
        "    min_v, max_v = int(u.min()), int(u.max())\n",
        "    per_file_stats.append((p, min_v, max_v, u[:20].tolist()))  # only show first 20 unique values\n",
        "\n",
        "    if (min_v < 0) or (max_v >= NUM_CLASSES):\n",
        "        bad_files.append((p, min_v, max_v))\n",
        "\n",
        "# --- print summary ---\n",
        "total_pixels = sum(global_counts.values())\n",
        "sorted_vals = sorted(global_counts.items(), key=lambda kv: kv[0])\n",
        "\n",
        "print(f\"Scanned {len(mask_paths)} mask files\")\n",
        "print(f\"Total pixels counted: {total_pixels:,}\")\n",
        "print(\"\\nGlobal label histogram (value: count, percent):\")\n",
        "for v, cnt in sorted_vals:\n",
        "    pct = 100.0 * cnt / (total_pixels + 1e-9)\n",
        "    print(f\"  {int(v):>3}: {cnt:>10}  ({pct:5.2f}%)\")\n",
        "\n",
        "print(\"\\nMin/Max over all files:\")\n",
        "all_vals = [v for v,_ in sorted_vals]\n",
        "print(f\"  global min: {min(all_vals)}\")\n",
        "print(f\"  global max: {max(all_vals)}\")\n",
        "\n",
        "if rgb_like:\n",
        "    print(f\"\\n[WARN] {len(rgb_like)} masks loaded as 3-channel (likely color). Examples:\")\n",
        "    for p in rgb_like[:5]:\n",
        "        print(\"  \", p)\n",
        "    print(\"→ Convert these to single-channel index masks (uint8 indices 0..NUM_CLASSES-1).\")\n",
        "\n",
        "if bad_files:\n",
        "    print(f\"\\n[PROBLEM] {len(bad_files)} files contain labels outside [0..{NUM_CLASSES-1}]. Examples:\")\n",
        "    for p, mn, mx in bad_files[:10]:\n",
        "        print(f\"  {p}  (min={mn}, max={mx})\")\n",
        "else:\n",
        "    print(f\"\\nAll files are within [0..{NUM_CLASSES-1}].\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9pcZatfNNBzv",
        "outputId": "71fe2a9e-dddb-4dc1-e075-d244fb4b9f95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scanned 3535 mask files\n",
            "Total pixels counted: 955,040,358\n",
            "\n",
            "Global label histogram (value: count, percent):\n",
            "    0:  892493471  (93.45%)\n",
            "    1:    7766168  ( 0.81%)\n",
            "    2:    8144924  ( 0.85%)\n",
            "    3:    7945922  ( 0.83%)\n",
            "    4:    7153831  ( 0.75%)\n",
            "    5:    6286986  ( 0.66%)\n",
            "    6:    4966257  ( 0.52%)\n",
            "    7:    2323574  ( 0.24%)\n",
            "    8:     576605  ( 0.06%)\n",
            "    9:      68376  ( 0.01%)\n",
            "   10:    8390497  ( 0.88%)\n",
            "   11:    1413412  ( 0.15%)\n",
            "   12:    1656320  ( 0.17%)\n",
            "   13:    1663399  ( 0.17%)\n",
            "   14:    1492928  ( 0.16%)\n",
            "   15:    1184436  ( 0.12%)\n",
            "   16:     877683  ( 0.09%)\n",
            "   17:     480197  ( 0.05%)\n",
            "   18:     127329  ( 0.01%)\n",
            "   19:      28043  ( 0.00%)\n",
            "\n",
            "Min/Max over all files:\n",
            "  global min: 0\n",
            "  global max: 19\n",
            "\n",
            "[PROBLEM] 3285 files contain labels outside [0..5]. Examples:\n",
            "  /content/mri_data_png/data/masks/100_01.png  (min=0, max=8)\n",
            "  /content/mri_data_png/data/masks/100_02.png  (min=0, max=11)\n",
            "  /content/mri_data_png/data/masks/100_03.png  (min=0, max=18)\n",
            "  /content/mri_data_png/data/masks/100_04.png  (min=0, max=18)\n",
            "  /content/mri_data_png/data/masks/100_05.png  (min=0, max=18)\n",
            "  /content/mri_data_png/data/masks/100_06.png  (min=0, max=18)\n",
            "  /content/mri_data_png/data/masks/100_07.png  (min=0, max=18)\n",
            "  /content/mri_data_png/data/masks/100_08.png  (min=0, max=18)\n",
            "  /content/mri_data_png/data/masks/100_09.png  (min=0, max=18)\n",
            "  /content/mri_data_png/data/masks/100_10.png  (min=0, max=17)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install spatialdata-io\n",
        "!wget https://s3.embl.de/spatialdata/spatialdata-sandbox/merfish.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKqYvZ4iPhdq",
        "outputId": "1119380f-638e-4119-c753-45b976623e63"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-09-05 07:22:14--  https://s3.embl.de/spatialdata/spatialdata-sandbox/merfish.zip\n",
            "Resolving s3.embl.de (s3.embl.de)... 194.94.45.80\n",
            "Connecting to s3.embl.de (s3.embl.de)|194.94.45.80|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 53533526 (51M) [application/zip]\n",
            "Saving to: ‘merfish.zip’\n",
            "\n",
            "merfish.zip         100%[===================>]  51.05M  19.4MB/s    in 2.6s    \n",
            "\n",
            "2025-09-05 07:22:17 (19.4 MB/s) - ‘merfish.zip’ saved [53533526/53533526]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/merfish.zip\"   # adjust path if different\n",
        "extract_path = \"/content/merfish\"\n",
        "\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Extracted to:\", extract_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PFp9AqqJwVS5",
        "outputId": "f5cfcc1c-6260-40b0-b0cf-d1be229ada32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracted to: /content/merfish\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import spatialdata as sd           # read_zarr\n",
        "import scanpy as sc                # transcriptomics core\n",
        "import squidpy as sq               # spatial graph/stats\n",
        "from shapely.geometry import shape # centroid fallback with GeoJSON dicts\n",
        "import pyarrow.parquet as pq       # read shapes/points parquet\n",
        "\n",
        "ZARR_DIR = Path(\"/content/merfish/data.zarr\")  # <-- change to absolute path if needed\n",
        "assert ZARR_DIR.exists(), f\"{ZARR_DIR} not found\"\n",
        "\n",
        "# -------------------------\n",
        "# 1) Load SpatialData Zarr\n",
        "# -------------------------\n",
        "sdata = sd.read_zarr(str(ZARR_DIR))\n",
        "\n",
        "# Pick the main table -> AnnData (cells x genes)\n",
        "# Most SpatialData stores name it \"table\"\n",
        "if \"table\" in sdata.tables:\n",
        "    adata = sdata.tables[\"table\"]\n",
        "else:\n",
        "    # take the first table if not named \"table\"\n",
        "    adata = next(iter(sdata.tables.values()))\n",
        "\n",
        "print(adata)\n",
        "print(\"obs columns:\", list(adata.obs.columns)[:10])\n",
        "print(\"var columns:\", list(adata.var.columns)[:10])\n",
        "print(\"obsm keys:\", list(adata.obsm.keys()))\n",
        "\n",
        "# ---------------------------------------------------\n",
        "# 2) Get per-cell spatial coordinates for visualization\n",
        "#    Priority:\n",
        "#    (A) adata.obsm['spatial'] if present\n",
        "#    (B) centroids from shapes/cells/shapes.parquet\n",
        "#    (C) centroids from points if points contain cell_id\n",
        "# ---------------------------------------------------\n",
        "coords = None\n",
        "\n",
        "# (A) common convention: 'spatial'\n",
        "if \"spatial\" in adata.obsm and adata.obsm[\"spatial\"] is not None:\n",
        "    XY = np.asarray(adata.obsm[\"spatial\"])\n",
        "    coords = pd.DataFrame({\"cell_id\": adata.obs_names.astype(str),\n",
        "                           \"x\": XY[:,0].astype(float),\n",
        "                           \"y\": XY[:,1].astype(float)})\n",
        "    print(\"Using coordinates from adata.obsm['spatial'].\")\n",
        "\n",
        "# (B) cell polygons -> centroids\n",
        "if coords is None:\n",
        "    shapes_cells = ZARR_DIR/\"shapes\"/\"cells\"/\"shapes.parquet\"\n",
        "    if shapes_cells.exists():\n",
        "        df_shapes = pq.read_table(shapes_cells).to_pandas()\n",
        "        # heuristics to find an id column\n",
        "        id_col = None\n",
        "        for k in [\"cell_id\",\"id\",\"label\",\"name\",\"_index\",\"index\"]:\n",
        "            if k in df_shapes.columns:\n",
        "                id_col = k; break\n",
        "        if id_col is None:\n",
        "            df_shapes = df_shapes.reset_index().rename(columns={\"index\":\"cell_id\"})\n",
        "            id_col = \"cell_id\"\n",
        "\n",
        "        # If geometry is present in WKB/GeoJSON-like dicts\n",
        "        if \"geometry\" in df_shapes.columns:\n",
        "            # geometry may be dict (GeoJSON) or bytes (WKB). Handle both:\n",
        "            try:\n",
        "                cent = []\n",
        "                for g in df_shapes[\"geometry\"]:\n",
        "                    if isinstance(g, dict):\n",
        "                        c = shape(g).centroid\n",
        "                    elif isinstance(g, (bytes, bytearray, memoryview)):\n",
        "                        from shapely import wkb\n",
        "                        c = wkb.loads(bytes(g)).centroid\n",
        "                    else:\n",
        "                        # unknown geometry encoding; skip\n",
        "                        c = None\n",
        "                    cent.append((float(c.x), float(c.y)) if c is not None else (np.nan, np.nan))\n",
        "                cent = np.array(cent)\n",
        "                coords = pd.DataFrame({\"cell_id\": df_shapes[id_col].astype(str),\n",
        "                                       \"x\": cent[:,0], \"y\": cent[:,1]})\n",
        "                coords = coords.dropna()\n",
        "                print(\"Using coordinates from shapes/cells centroids.\")\n",
        "            except Exception as e:\n",
        "                print(\"Could not parse shapes geometry:\", e)\n",
        "\n",
        "# (C) points with cell_id -> mean per cell_id\n",
        "if coords is None:\n",
        "    points_dir = ZARR_DIR/\"points\"/\"single_molecule\"/\"points.parquet\"\n",
        "    if points_dir.exists():\n",
        "        pts = pq.read_table(points_dir).to_pandas()\n",
        "        cols = {c.lower(): c for c in pts.columns}\n",
        "        if {\"cell_id\",\"x\",\"y\"}.issubset(cols.keys()):\n",
        "            cell_col, x_col, y_col = cols[\"cell_id\"], cols[\"x\"], cols[\"y\"]\n",
        "            coords = (pts.groupby(cell_col)[[x_col,y_col]]\n",
        "                        .mean().reset_index()\n",
        "                        .rename(columns={cell_col:\"cell_id\", x_col:\"x\", y_col:\"y\"}))\n",
        "            print(\"Using coordinates from points (mean x,y per cell_id).\")\n",
        "        else:\n",
        "            print(\"Points found but missing 'cell_id' -> cannot derive per-cell coords from points.\")\n",
        "\n",
        "# Save coords csv if available (useful for debugging or plotting elsewhere)\n",
        "if coords is not None:\n",
        "    coords.to_csv(\"coords.csv\", index=False)\n",
        "    print(\"coords.csv saved:\", coords.shape)\n",
        "else:\n",
        "    print(\"WARNING: No per-cell coordinates found. Spatial plots will be skipped.\")\n",
        "\n",
        "# ---------------------------------------\n",
        "# 3) Export counts.csv (cells x genes)\n",
        "# ---------------------------------------\n",
        "counts = pd.DataFrame(adata.X.toarray() if hasattr(adata.X, \"toarray\") else adata.X,\n",
        "                      index=adata.obs_names, columns=adata.var_names)\n",
        "counts.reset_index().rename(columns={\"index\":\"cell_id\"}).to_csv(\"counts.csv\", index=False)\n",
        "print(\"counts.csv saved:\", counts.shape)\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 4) Minimal analysis: QC -> normalize/log -> PCA -> UMAP/Leiden\n",
        "# -------------------------------------------------------\n",
        "sc.pp.calculate_qc_metrics(adata, inplace=True)  # n_genes_by_counts, total_counts, etc.\n",
        "\n",
        "# Simple filtering suggestions (optional)\n",
        "# sc.pp.filter_cells(adata, min_genes=100)\n",
        "# sc.pp.filter_genes(adata, min_cells=3)\n",
        "\n",
        "sc.pp.normalize_total(adata, target_sum=1e4)\n",
        "sc.pp.log1p(adata)\n",
        "sc.pp.highly_variable_genes(adata, n_top_genes=2000, flavor=\"seurat_v3\", subset=True)\n",
        "sc.pp.scale(adata, max_value=10)\n",
        "sc.tl.pca(adata, n_comps=50)\n",
        "sc.pp.neighbors(adata, n_neighbors=15, n_pcs=30)\n",
        "sc.tl.umap(adata)\n",
        "sc.tl.leiden(adata, key_added=\"cluster\")\n",
        "\n",
        "# UMAP colored by cluster\n",
        "sc.pl.umap(adata, color=[\"cluster\"], legend_loc=\"on data\", show=False)\n",
        "plt.savefig(\"umap_clusters.png\", dpi=150, bbox_inches=\"tight\"); plt.close()\n",
        "print(\"Saved: umap_clusters.png\")\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 5) Spatial analysis (only if we have per-cell coords)\n",
        "# -------------------------------------------------------\n",
        "if coords is not None:\n",
        "    # align order to adata.obs\n",
        "    coords_indexed = coords.set_index(\"cell_id\").reindex(adata.obs_names)\n",
        "    # store as generic coordinates\n",
        "    adata.obsm[\"spatial\"] = coords_indexed[[\"x\",\"y\"]].values\n",
        "\n",
        "    # Build spatial neighbor graph (generic 2D coordinates)\n",
        "    sq.gr.spatial_neighbors(adata, coord_type=\"generic\")\n",
        "    # Neighborhood enrichment between clusters\n",
        "    sq.gr.nhood_enrichment(adata, cluster_key=\"cluster\")\n",
        "\n",
        "    # Quick spatial scatter (matplotlib)\n",
        "    fig, ax = plt.subplots()\n",
        "    sc = ax.scatter(adata.obsm[\"spatial\"][:,0], adata.obsm[\"spatial\"][:,1],\n",
        "                    c=pd.Categorical(adata.obs[\"cluster\"]).codes, s=6)\n",
        "    ax.set_title(\"Spatial clusters (scatter)\")\n",
        "    ax.set_aspect(\"equal\")\n",
        "    ax.invert_yaxis()  # image-like coords\n",
        "    plt.savefig(\"spatial_clusters.png\", dpi=200, bbox_inches=\"tight\"); plt.close()\n",
        "    print(\"Saved: spatial_clusters.png\")\n",
        "\n",
        "    # Neighborhood enrichment heatmap\n",
        "    sq.pl.nhood_enrichment(adata, cluster_key=\"cluster\", figsize=(5,4), show=False)\n",
        "    plt.savefig(\"nhood_enrichment.png\", dpi=150, bbox_inches=\"tight\"); plt.close()\n",
        "    print(\"Saved: nhood_enrichment.png\")\n",
        "else:\n",
        "    print(\"Skipped spatial plots: no coords.\")\n",
        "\n",
        "# -------------------------------------------------------\n",
        "# 6) Simple marker discovery per cluster (Δ-mean quick pass)\n",
        "# -------------------------------------------------------\n",
        "clusters = adata.obs[\"cluster\"].astype(str)\n",
        "X = (adata.X.toarray() if hasattr(adata.X, \"toarray\") else adata.X)\n",
        "import numpy as np\n",
        "markers = []\n",
        "for c in sorted(clusters.unique()):\n",
        "    mask = (clusters == c).values\n",
        "    mu_in  = np.asarray(X[mask,:]).mean(axis=0)\n",
        "    mu_out = np.asarray(X[~mask,:]).mean(axis=0)\n",
        "    delta = mu_in - mu_out\n",
        "    top_idx = np.argsort(delta)[::-1][:10]\n",
        "    for rank, j in enumerate(top_idx, start=1):\n",
        "        markers.append({\"cluster\": c, \"rank\": rank, \"gene\": adata.var_names[j], \"delta_mean\": float(delta[j])})\n",
        "pd.DataFrame(markers).to_csv(\"cluster_markers_top10.csv\", index=False)\n",
        "print(\"Saved: cluster_markers_top10.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 689
        },
        "id": "1J-UGAkRwhak",
        "outputId": "0c44901f-5075-45b9-a98c-b01597911c98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/squidpy/im/_io.py:12: FutureWarning: `find_available_plugins` is deprecated since version 0.25 and will be removed in version 0.27. The plugin infrastructure of `skimage.io` is deprecated. Instead, use `imageio` or other I/O packages directly.\n",
            "  from skimage.io import imread\n",
            "/usr/local/lib/python3.12/dist-packages/squidpy/im/_container.py:47: FutureWarning: `reset_plugins` is deprecated since version 0.25 and will be removed in version 0.27. The plugin infrastructure of `skimage.io` is deprecated. Instead, use `imageio` or other I/O packages directly.\n",
            "  from squidpy.im._io import _assert_dims_present, _infer_dimensions, _lazy_load_image\n",
            "/usr/local/lib/python3.12/dist-packages/squidpy/read/_read.py:17: FutureWarning: Importing read_text from `anndata` is deprecated. Import anndata.io.read_text instead.\n",
            "  from squidpy.read._utils import _load_image, _read_counts\n",
            "WARNING:ome_zarr.io:version mismatch: detected: RasterFormatV02, requested: FormatV04\n",
            "/usr/local/lib/python3.12/dist-packages/zarr/creation.py:610: UserWarning: ignoring keyword argument 'read_only'\n",
            "  compressor, fill_value = _kwargs_compat(compressor, fill_value, kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AnnData object with n_obs × n_vars = 2389 × 268\n",
            "    obs: 'cell_id', 'region'\n",
            "    uns: 'spatialdata_attrs'\n",
            "obs columns: ['cell_id', 'region']\n",
            "var columns: []\n",
            "obsm keys: []\n",
            "Using coordinates from shapes/cells centroids.\n",
            "coords.csv saved: (2389, 3)\n",
            "counts.csv saved: (2389, 268)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "Positions outside range of features.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2891204511.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;31m# 4) Minimal analysis: QC -> normalize/log -> PCA -> UMAP/Leiden\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;31m# -------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalculate_qc_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# n_genes_by_counts, total_counts, etc.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;31m# Simple filtering suggestions (optional)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scanpy/preprocessing/_qc.py\u001b[0m in \u001b[0;36mcalculate_qc_metrics\u001b[0;34m(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, inplace, log1p, parallel)\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mqc_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mqc_vars\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     obs_metrics = describe_obs(\n\u001b[0m\u001b[1;32m    314\u001b[0m         \u001b[0madata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mexpr_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpr_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scanpy/preprocessing/_qc.py\u001b[0m in \u001b[0;36mdescribe_obs\u001b[0;34m(adata, expr_type, var_type, qc_vars, percent_top, layer, use_raw, log1p, inplace, X, parallel)\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mpercent_top\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0mpercent_top\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpercent_top\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m         \u001b[0mproportions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtop_segment_proportions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpercent_top\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpercent_top\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             obs_metrics[f\"pct_{expr_type}_in_top_{n}_{var_type}\"] = (\n",
            "\u001b[0;32m/usr/lib/python3.12/functools.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    910\u001b[0m                             '1 positional argument')\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mdispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0mfuncname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__name__'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'singledispatch function'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/scanpy/preprocessing/_qc.py\u001b[0m in \u001b[0;36mcheck_ns_inner\u001b[0;34m(mtx, ns)\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmtx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mns\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m             \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Positions outside range of features.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    395\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Positions outside range of features."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# (recommended) conda create -n sdata python=3.10 -y && conda activate sdata\n",
        "!python -m pip install -U pip\n",
        "!python -m pip install spatialdata scanpy squidpy zarr pyarrow shapely geopandas matplotlib\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ryBfHjaewioI",
        "outputId": "f559a869-0ee4-4ea5-d02d-7e007c83465a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.2-py3-none-any.whl.metadata (4.7 kB)\n",
            "Downloading pip-25.2-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.2\n",
            "Requirement already satisfied: spatialdata in /usr/local/lib/python3.12/dist-packages (0.5.0)\n",
            "Requirement already satisfied: scanpy in /usr/local/lib/python3.12/dist-packages (1.11.4)\n",
            "Collecting squidpy\n",
            "  Downloading squidpy-1.6.5-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: zarr in /usr/local/lib/python3.12/dist-packages (2.18.7)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (18.1.0)\n",
            "Requirement already satisfied: shapely in /usr/local/lib/python3.12/dist-packages (2.1.1)\n",
            "Requirement already satisfied: geopandas in /usr/local/lib/python3.12/dist-packages (1.1.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: anndata>=0.9.1 in /usr/local/lib/python3.12/dist-packages (from spatialdata) (0.12.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from spatialdata) (8.2.1)\n",
            "Requirement already satisfied: dask-image in /usr/local/lib/python3.12/dist-packages (from spatialdata) (2024.5.3)\n",
            "Requirement already satisfied: dask<=2024.11.2,>=2024.4.1 in /usr/local/lib/python3.12/dist-packages (from spatialdata) (2024.11.2)\n",
            "Requirement already satisfied: datashader in /usr/local/lib/python3.12/dist-packages (from spatialdata) (0.18.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from spatialdata) (2025.3.0)\n",
            "Requirement already satisfied: multiscale-spatial-image>=2.0.3 in /usr/local/lib/python3.12/dist-packages (from spatialdata) (2.0.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from spatialdata) (3.5)\n",
            "Requirement already satisfied: numba>=0.55.0 in /usr/local/lib/python3.12/dist-packages (from spatialdata) (0.60.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from spatialdata) (2.0.2)\n",
            "Requirement already satisfied: ome-zarr>=0.8.4 in /usr/local/lib/python3.12/dist-packages (from spatialdata) (0.11.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from spatialdata) (2.2.2)\n",
            "Requirement already satisfied: pooch in /usr/local/lib/python3.12/dist-packages (from spatialdata) (1.8.2)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from spatialdata) (13.9.4)\n",
            "Requirement already satisfied: scikit-image in /usr/local/lib/python3.12/dist-packages (from spatialdata) (0.25.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from spatialdata) (1.16.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from spatialdata) (75.2.0)\n",
            "Requirement already satisfied: spatial-image>=1.2.3 in /usr/local/lib/python3.12/dist-packages (from spatialdata) (1.2.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.12/dist-packages (from spatialdata) (4.15.0)\n",
            "Requirement already satisfied: xarray-schema in /usr/local/lib/python3.12/dist-packages (from spatialdata) (0.0.3)\n",
            "Requirement already satisfied: xarray-spatial>=0.3.5 in /usr/local/lib/python3.12/dist-packages (from spatialdata) (0.4.0)\n",
            "Requirement already satisfied: xarray>=2024.10.0 in /usr/local/lib/python3.12/dist-packages (from spatialdata) (2025.8.0)\n",
            "Requirement already satisfied: asciitree in /usr/local/lib/python3.12/dist-packages (from zarr) (0.3.3)\n",
            "Requirement already satisfied: fasteners in /usr/local/lib/python3.12/dist-packages (from zarr) (0.20)\n",
            "Requirement already satisfied: numcodecs!=0.14.0,!=0.14.1,<0.16,>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from zarr) (0.15.1)\n",
            "Requirement already satisfied: cloudpickle>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from dask<=2024.11.2,>=2024.4.1->spatialdata) (3.1.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from dask<=2024.11.2,>=2024.4.1->spatialdata) (25.0)\n",
            "Requirement already satisfied: partd>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from dask<=2024.11.2,>=2024.4.1->spatialdata) (1.4.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from dask<=2024.11.2,>=2024.4.1->spatialdata) (6.0.2)\n",
            "Requirement already satisfied: toolz>=0.10.0 in /usr/local/lib/python3.12/dist-packages (from dask<=2024.11.2,>=2024.4.1->spatialdata) (0.12.1)\n",
            "Requirement already satisfied: deprecated in /usr/local/lib/python3.12/dist-packages (from numcodecs!=0.14.0,!=0.14.1,<0.16,>=0.10.0->zarr) (1.2.18)\n",
            "Requirement already satisfied: h5py>=3.7.0 in /usr/local/lib/python3.12/dist-packages (from scanpy) (3.14.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from scanpy) (1.5.2)\n",
            "Requirement already satisfied: legacy-api-wrap>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from scanpy) (1.4.1)\n",
            "Requirement already satisfied: natsort in /usr/local/lib/python3.12/dist-packages (from scanpy) (8.4.0)\n",
            "Requirement already satisfied: patsy!=1.0.0 in /usr/local/lib/python3.12/dist-packages (from scanpy) (1.0.1)\n",
            "Requirement already satisfied: pynndescent>=0.5.13 in /usr/local/lib/python3.12/dist-packages (from scanpy) (0.5.13)\n",
            "Requirement already satisfied: scikit-learn>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from scanpy) (1.6.1)\n",
            "Requirement already satisfied: seaborn>=0.13.2 in /usr/local/lib/python3.12/dist-packages (from scanpy) (0.13.2)\n",
            "Requirement already satisfied: session-info2 in /usr/local/lib/python3.12/dist-packages (from scanpy) (0.2.1)\n",
            "Requirement already satisfied: statsmodels>=0.14.5 in /usr/local/lib/python3.12/dist-packages (from scanpy) (0.14.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from scanpy) (4.67.1)\n",
            "Requirement already satisfied: umap-learn>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from scanpy) (0.5.9.post2)\n",
            "Requirement already satisfied: aiohttp>=3.8.1 in /usr/local/lib/python3.12/dist-packages (from squidpy) (3.12.15)\n",
            "Requirement already satisfied: cycler>=0.11.0 in /usr/local/lib/python3.12/dist-packages (from squidpy) (0.12.1)\n",
            "Collecting docrep>=0.3.1 (from squidpy)\n",
            "  Downloading docrep-0.3.2.tar.gz (33 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting matplotlib-scalebar>=0.8.0 (from squidpy)\n",
            "  Downloading matplotlib_scalebar-0.9.0-py3-none-any.whl.metadata (18 kB)\n",
            "Collecting omnipath>=1.0.7 (from squidpy)\n",
            "  Downloading omnipath-1.0.12-py3-none-any.whl.metadata (7.0 kB)\n",
            "Requirement already satisfied: pillow>=8.0.0 in /usr/local/lib/python3.12/dist-packages (from squidpy) (11.3.0)\n",
            "Requirement already satisfied: tifffile!=2022.4.22 in /usr/local/lib/python3.12/dist-packages (from squidpy) (2025.8.28)\n",
            "Collecting validators>=0.18.2 (from squidpy)\n",
            "  Downloading validators-0.35.0-py3-none-any.whl.metadata (3.9 kB)\n",
            "Requirement already satisfied: pyogrio>=0.7.2 in /usr/local/lib/python3.12/dist-packages (from geopandas) (0.11.1)\n",
            "Requirement already satisfied: pyproj>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from geopandas) (3.7.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8.1->squidpy) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8.1->squidpy) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8.1->squidpy) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8.1->squidpy) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8.1->squidpy) (6.6.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8.1->squidpy) (0.3.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp>=3.8.1->squidpy) (1.20.1)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp>=3.8.1->squidpy) (3.10)\n",
            "Requirement already satisfied: array-api-compat>=1.7.1 in /usr/local/lib/python3.12/dist-packages (from anndata>=0.9.1->spatialdata) (1.12.0)\n",
            "Requirement already satisfied: pims>=0.4.1 in /usr/local/lib/python3.12/dist-packages (from dask-image->spatialdata) (0.7)\n",
            "Requirement already satisfied: dask-expr<1.2,>=1.1 in /usr/local/lib/python3.12/dist-packages (from dask<=2024.11.2,>=2024.4.1->spatialdata) (1.1.19)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from docrep>=0.3.1->squidpy) (1.17.0)\n",
            "Requirement already satisfied: xarray-dataclass>=3.0.0 in /usr/local/lib/python3.12/dist-packages (from multiscale-spatial-image>=2.0.3->spatialdata) (3.0.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.55.0->spatialdata) (0.43.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from ome-zarr>=0.8.4->spatialdata) (2.32.4)\n",
            "Requirement already satisfied: s3fs in /usr/local/lib/python3.12/dist-packages (from fsspec[s3]!=2021.07.0,!=2023.9.0,>=0.8->ome-zarr>=0.8.4->spatialdata) (2025.3.0)\n",
            "Requirement already satisfied: inflect>=4.1.0 in /usr/local/lib/python3.12/dist-packages (from omnipath>=1.0.7->squidpy) (7.5.0)\n",
            "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from omnipath>=1.0.7->squidpy) (2.5.0)\n",
            "Requirement already satisfied: wrapt>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from omnipath>=1.0.7->squidpy) (1.17.3)\n",
            "Requirement already satisfied: more_itertools>=8.5.0 in /usr/local/lib/python3.12/dist-packages (from inflect>=4.1.0->omnipath>=1.0.7->squidpy) (10.8.0)\n",
            "Requirement already satisfied: typeguard>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from inflect>=4.1.0->omnipath>=1.0.7->squidpy) (4.4.4)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->spatialdata) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->spatialdata) (2025.2)\n",
            "Requirement already satisfied: locket in /usr/local/lib/python3.12/dist-packages (from partd>=1.4.0->dask<=2024.11.2,>=2024.4.1->spatialdata) (1.0.0)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.12/dist-packages (from pims>=0.4.1->dask-image->spatialdata) (2.37.0)\n",
            "Requirement already satisfied: slicerator>=0.9.8 in /usr/local/lib/python3.12/dist-packages (from pims>=0.4.1->dask-image->spatialdata) (1.1.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from pyogrio>=0.7.2->geopandas) (2025.8.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->ome-zarr>=0.8.4->spatialdata) (3.4.3)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image->spatialdata) (0.4)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=1.1.3->scanpy) (3.6.0)\n",
            "Requirement already satisfied: colorcet in /usr/local/lib/python3.12/dist-packages (from datashader->spatialdata) (3.1.0)\n",
            "Requirement already satisfied: multipledispatch in /usr/local/lib/python3.12/dist-packages (from datashader->spatialdata) (1.0.0)\n",
            "Requirement already satisfied: param in /usr/local/lib/python3.12/dist-packages (from datashader->spatialdata) (2.2.1)\n",
            "Requirement already satisfied: pyct in /usr/local/lib/python3.12/dist-packages (from datashader->spatialdata) (0.5.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from pooch->spatialdata) (4.4.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->spatialdata) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->spatialdata) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->spatialdata) (0.1.2)\n",
            "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /usr/local/lib/python3.12/dist-packages (from s3fs->fsspec[s3]!=2021.07.0,!=2023.9.0,>=0.8->ome-zarr>=0.8.4->spatialdata) (2.24.1)\n",
            "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->fsspec[s3]!=2021.07.0,!=2023.9.0,>=0.8->ome-zarr>=0.8.4->spatialdata) (0.12.0)\n",
            "Requirement already satisfied: botocore<1.39.12,>=1.39.9 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->fsspec[s3]!=2021.07.0,!=2023.9.0,>=0.8->ome-zarr>=0.8.4->spatialdata) (1.39.11)\n",
            "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs->fsspec[s3]!=2021.07.0,!=2023.9.0,>=0.8->ome-zarr>=0.8.4->spatialdata) (1.0.1)\n",
            "Downloading squidpy-1.6.5-py3-none-any.whl (161 kB)\n",
            "Downloading matplotlib_scalebar-0.9.0-py3-none-any.whl (16 kB)\n",
            "Downloading omnipath-1.0.12-py3-none-any.whl (51 kB)\n",
            "Downloading validators-0.35.0-py3-none-any.whl (44 kB)\n",
            "Building wheels for collected packages: docrep\n",
            "\u001b[33m  DEPRECATION: Building 'docrep' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'docrep'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for docrep (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for docrep: filename=docrep-0.3.2-py3-none-any.whl size=19876 sha256=ec4a168f3437891ceac1bb6e044a928a6de22c02582e84952b2345e60cf7916a\n",
            "  Stored in directory: /root/.cache/pip/wheels/d6/19/ee/0a6a1793d91c449563b49ccab57ce52da3e6fab7614836bd8c\n",
            "Successfully built docrep\n",
            "Installing collected packages: validators, docrep, omnipath, matplotlib-scalebar, squidpy\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5/5\u001b[0m [squidpy]\n",
            "\u001b[1A\u001b[2KSuccessfully installed docrep-0.3.2 matplotlib-scalebar-0.9.0 omnipath-1.0.12 squidpy-1.6.5 validators-0.35.0\n"
          ]
        }
      ]
    }
  ]
}