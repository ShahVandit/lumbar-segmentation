{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjLZGBHDJj1q",
        "outputId": "c95f73fa-8297-4b4a-8fce-fb61853fc475"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting opencv-python\n",
            "  Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy<2.3.0,>=2 in /usr/local/lib/python3.12/dist-packages (from opencv-python) (2.0.2)\n",
            "Downloading opencv_python-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (67.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opencv-python\n",
            "Successfully installed opencv-python-4.12.0.88\n",
            "Collecting albumentations\n",
            "  Downloading albumentations-2.0.8-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.1/43.1 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.12/dist-packages (from albumentations) (1.16.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from albumentations) (6.0.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.12/dist-packages (from albumentations) (2.11.7)\n",
            "Collecting albucore==0.0.24 (from albumentations)\n",
            "  Downloading albucore-0.0.24-py3-none-any.whl.metadata (5.3 kB)\n",
            "Collecting opencv-python-headless>=4.9.0.80 (from albumentations)\n",
            "  Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (19 kB)\n",
            "Collecting stringzilla>=3.10.4 (from albucore==0.0.24->albumentations)\n",
            "  Downloading stringzilla-3.12.6-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl.metadata (80 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting simsimd>=5.9.2 (from albucore==0.0.24->albumentations)\n",
            "  Downloading simsimd-6.5.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (70 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.5/70.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (4.15.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.1)\n",
            "Downloading albumentations-2.0.8-py3-none-any.whl (369 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m369.4/369.4 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading albucore-0.0.24-py3-none-any.whl (15 kB)\n",
            "Downloading opencv_python_headless-4.12.0.88-cp37-abi3-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (54.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading simsimd-6.5.1-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m67.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading stringzilla-3.12.6-cp312-cp312-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_28_x86_64.whl (308 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m308.4/308.4 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: stringzilla, simsimd, opencv-python-headless, albucore, albumentations\n",
            "Successfully installed albucore-0.0.24 albumentations-2.0.8 opencv-python-headless-4.12.0.88 simsimd-6.5.1 stringzilla-3.12.6\n",
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# !pip install google-colab\n",
        "!pip install opencv-python\n",
        "!pip install -U albumentations\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TThf10XpKORZ",
        "outputId": "aad8a5dd-36fc-4040-bd07-bc8e0cad0943"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted to: /content/mri_data_png\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "zip_path = \"/content/drive/MyDrive/mri_data_png.zip\"\n",
        "extract_path = \"/content/mri_data_png\"\n",
        "\n",
        "os.makedirs(extract_path, exist_ok=True)\n",
        "\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "\n",
        "print(\"Extracted to:\", extract_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTLU5qE8JBtQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, down=True, act=\"relu\", use_dropout=False, norm=\"instance\"):\n",
        "        super(Block, self).__init__()\n",
        "\n",
        "        conv = (\n",
        "            nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False, padding_mode=\"reflect\")\n",
        "            if down else\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False)\n",
        "        )\n",
        "\n",
        "        # Choose normalization\n",
        "        if norm == \"instance\":\n",
        "            norm_layer = nn.InstanceNorm2d(out_channels, affine=True)\n",
        "        elif norm == \"batch\":\n",
        "            norm_layer = nn.BatchNorm2d(out_channels)\n",
        "        elif norm is None:\n",
        "            norm_layer = nn.Identity()\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown norm='{norm}'. Use 'instance', 'batch', or None.\")\n",
        "\n",
        "        act_layer = nn.ReLU(inplace=True) if act == \"relu\" else nn.LeakyReLU(0.2, inplace=True)\n",
        "\n",
        "        self.conv = nn.Sequential(conv, norm_layer, act_layer)\n",
        "        self.use_dropout = use_dropout\n",
        "        self.dropout = nn.Dropout2d(0.5) \n",
        "\n",
        "        self.down = down\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv(x)\n",
        "        return self.dropout(x) if self.use_dropout else x\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, in_ch=1, out_ch=6, features=64):\n",
        "        super().__init__()\n",
        "        # Encoder: 256->128->64->32->16->8->4->2  (initial_down + 6 downs)\n",
        "        self.initial_down = nn.Sequential(\n",
        "            nn.Conv2d(in_ch, features, 4, 2, 1, padding_mode=\"reflect\"),  # 256 -> 128\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "        self.down1 = Block(features,       features * 2, down=True, act=\"leaky\")   # 128 -> 64\n",
        "        self.down2 = Block(features * 2,   features * 4, down=True, act=\"leaky\")   # 64  -> 32\n",
        "        self.down3 = Block(features * 4,   features * 8, down=True, act=\"leaky\")   # 32  -> 16\n",
        "        self.down4 = Block(features * 8,   features * 8, down=True, act=\"leaky\")   # 16  -> 8\n",
        "        self.down5 = Block(features * 8,   features * 8, down=True, act=\"leaky\")   # 8   -> 4\n",
        "        self.down6 = Block(features * 8,   features * 8, down=True, act=\"leaky\")   # 4   -> 2\n",
        "\n",
        "        # Bottleneck: 2x2 -> 1x1\n",
        "        self.bottleneck = nn.Sequential(\n",
        "            nn.Conv2d(features * 8, features * 8, 4, 2, 1),  # 2 -> 1\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "        self.up1 = Block(features * 8,       features * 8, down=False, act=\"relu\", use_dropout=True)\n",
        "        self.up2 = Block(features * 8 * 2,   features * 8, down=False, act=\"relu\", use_dropout=True)       \n",
        "        self.up3 = Block(features * 8 * 2,   features * 8, down=False, act=\"relu\", use_dropout=True)       \n",
        "        self.up4 = Block(features * 8 * 2,   features * 8, down=False, act=\"relu\")                         \n",
        "        self.up5 = Block(features * 8 * 2,   features * 4, down=False, act=\"relu\")                      \n",
        "        self.up6 = Block(features * 4 * 2,   features * 2, down=False, act=\"relu\")                          \n",
        "        self.up7 = Block(features * 2 * 2,   features,     down=False, act=\"relu\")                       \n",
        "\n",
        "        # Final: 128 -> 256, logits for 6 classes\n",
        "        self.final_up = nn.ConvTranspose2d(features * 2, out_ch, kernel_size=4, stride=2, padding=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        d1 = self.initial_down(x)   \n",
        "        d2 = self.down1(d1)         \n",
        "        d3 = self.down2(d2)    \n",
        "        d4 = self.down3(d3) \n",
        "        d5 = self.down4(d4)         \n",
        "        d6 = self.down5(d5)       \n",
        "        d7 = self.down6(d6)        \n",
        "\n",
        "        b  = self.bottleneck(d7)    # 1\n",
        "\n",
        "        u1 = self.up1(b)                        \n",
        "        u2 = self.up2(torch.cat([u1, d7], 1))     \n",
        "        u3 = self.up3(torch.cat([u2, d6], 1))      \n",
        "        u4 = self.up4(torch.cat([u3, d5], 1))     \n",
        "        u5 = self.up5(torch.cat([u4, d4], 1))   \n",
        "        u6 = self.up6(torch.cat([u5, d3], 1))     \n",
        "        u7 = self.up7(torch.cat([u6, d2], 1))     \n",
        "\n",
        "        logits = self.final_up(torch.cat([u7, d1], 1)) \n",
        "        return logits\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-v2fxkcqJGZR",
        "outputId": "5a8eaef8-5db4-45f9-e0a3-ec4df1980f8b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input shape : torch.Size([2, 1, 256, 256])\n",
            "Output shape: torch.Size([2, 6, 256, 256])\n"
          ]
        }
      ],
      "source": [
        "G = Generator(in_ch=1, out_ch=6, features=64)\n",
        "\n",
        "# Dummy input\n",
        "x = torch.randn(2, 1, 256, 256)\n",
        "\n",
        "# Forward pass\n",
        "y = G(x)\n",
        "\n",
        "print(\"Input shape :\", x.shape)\n",
        "print(\"Output shape:\", y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "omd0AX2oJIPH",
        "outputId": "e86f54b2-a2cb-4711-b3d0-0d94126470f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "D(x, y_idx) -> torch.Size([2, 1, 30, 30])\n",
            "D(x, y_probs) -> torch.Size([2, 1, 30, 30])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class CNNBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "        super().__init__()\n",
        "        self.conv = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, out_channels, 4, stride, 1, bias=False, padding_mode=\"reflect\"),\n",
        "            nn.InstanceNorm2d(out_channels, affine=True),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, in_ch_x=1, in_ch_y=6, features=(64, 128, 256, 512)):\n",
        "        super().__init__()\n",
        "        self.in_ch_x = in_ch_x\n",
        "        self.in_ch_y = in_ch_y\n",
        "\n",
        "        in_pair = in_ch_x + in_ch_y\n",
        "\n",
        "        self.initial = nn.Sequential(\n",
        "            nn.Conv2d(in_pair, features[0], kernel_size=4, stride=2, padding=1, padding_mode=\"reflect\"),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "        )\n",
        "\n",
        "        blocks = []\n",
        "        in_c = features[0]\n",
        "        for f in features[1:]:\n",
        "            stride = 1 if f == features[-1] else 2\n",
        "            blocks.append(nn.Sequential(\n",
        "                nn.Conv2d(in_c, f, kernel_size=4, stride=stride, padding=1, bias=False, padding_mode=\"reflect\"),\n",
        "                nn.InstanceNorm2d(f, affine=True),\n",
        "                nn.LeakyReLU(0.2, inplace=True),\n",
        "            ))\n",
        "            in_c = f\n",
        "\n",
        "        # Final 1-channel conv -> patch score map\n",
        "        blocks.append(nn.Conv2d(in_c, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"))\n",
        "\n",
        "        self.model = nn.Sequential(*blocks)\n",
        "\n",
        "    def _ensure_mask_channels(self, y):\n",
        "        \"\"\"\n",
        "        Convert (N,H,W) index mask to (N,C,H,W) one-hot if needed.\n",
        "        If y already has shape (N,C,H,W), return as-is.\n",
        "        \"\"\"\n",
        "        if y.dim() == 3:\n",
        "            # y is class indices\n",
        "            if not (y.dtype == torch.long or y.dtype == torch.int64):\n",
        "                y = y.long()\n",
        "            y = F.one_hot(y, num_classes=self.in_ch_y).permute(0, 3, 1, 2).float()\n",
        "        elif y.dim() == 4:\n",
        "            if y.size(1) != self.in_ch_y:\n",
        "                raise ValueError(f\"Expected mask with {self.in_ch_y} channels, got {y.size(1)}.\")\n",
        "            if not y.is_floating_point():\n",
        "                y = y.float()\n",
        "        else:\n",
        "            raise ValueError(f\"Mask y must be (N,H,W) or (N,C,H,W), got shape {tuple(y.shape)}.\")\n",
        "        return y\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        \"\"\"\n",
        "        x: (N, in_ch_x, H, W)\n",
        "        y: (N, in_ch_y, H, W)  or  (N, H, W) indices\n",
        "        \"\"\"\n",
        "        y = self._ensure_mask_channels(y)\n",
        "        pair = torch.cat([x, y], dim=1)        # (N, in_ch_x+in_ch_y, H, W)\n",
        "        h = self.initial(pair)\n",
        "        out = self.model(h)\n",
        "        return out\n",
        "# quick sanity test for your shapes\n",
        "if __name__ == \"__main__\":\n",
        "    N, H, W = 2, 256, 256\n",
        "    D = Discriminator(in_ch_x=1, in_ch_y=6)\n",
        "\n",
        "    x = torch.randn(N, 1, H, W)         \n",
        "    y_idx = torch.randint(0, 6, (N, H, W))    # indices\n",
        "    out_real = D(x, y_idx)\n",
        "    print(\"D(x, y_idx) ->\", out_real.shape)\n",
        "\n",
        "    y_probs = torch.softmax(torch.randn(N, 6, H, W), dim=1)\n",
        "    out_fake = D(x, y_probs)\n",
        "    print(\"D(x, y_probs) ->\", out_fake.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rQwjTodJKEn",
        "outputId": "b4cc0fcf-5502-43d0-e79b-bd16b09f8822"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:666: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([4, 1, 256, 256]) torch.Size([4, 256, 256])\n",
            "3182\n",
            "353\n"
          ]
        }
      ],
      "source": [
        "import os, glob, random, cv2, numpy as np, torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "def collect_ids(root=\"mri_data\", img_dir=\"images\", mask_dir=\"masks\"):\n",
        "    imgs  = {os.path.splitext(os.path.basename(p))[0]: p\n",
        "             for p in glob.glob(os.path.join(root, img_dir, \"*.png\"))}\n",
        "    masks = {os.path.splitext(os.path.basename(p))[0]: p\n",
        "             for p in glob.glob(os.path.join(root, mask_dir, \"*.png\"))}\n",
        "    ids = sorted(list(set(imgs.keys()) & set(masks.keys())))\n",
        "    if not ids:\n",
        "        raise RuntimeError(\"No matching image/mask basenames found.\")\n",
        "    return ids, imgs, masks\n",
        "\n",
        "def split_ids(ids, val_ratio=0.1, seed=42):\n",
        "    random.Random(seed).shuffle(ids)\n",
        "    n_val = max(1, int(len(ids) * val_ratio))\n",
        "    return ids[n_val:], ids[:n_val]  # train_ids, val_ids\n",
        "\n",
        "# ---------- dataset ----------\n",
        "class SpinePNG(Dataset):\n",
        "    def __init__(self, ids, img_map, mask_map):\n",
        "        self.ids = ids\n",
        "        self.img_map = img_map\n",
        "        self.mask_map = mask_map\n",
        "        self.tf = A.Compose([\n",
        "            A.Resize(256, 256),\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.Normalize(mean=(0.5,), std=(0.5,)),\n",
        "            ToTensorV2()\n",
        "        ])\n",
        "\n",
        "    def __len__(self): return len(self.ids)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        bid = self.ids[i]\n",
        "        img = cv2.imread(self.img_map[bid], cv2.IMREAD_UNCHANGED)\n",
        "        msk = cv2.imread(self.mask_map[bid], cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "        if img.ndim == 3:\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "        if img.dtype == np.uint16:\n",
        "            img = img.astype(np.float32) / 65535.0\n",
        "        else:\n",
        "            img = img.astype(np.float32) / 255.0\n",
        "\n",
        "        # albumentations expects HxW\n",
        "        aug = self.tf(image=img, mask=msk.astype(np.int64))\n",
        "        x = aug[\"image\"]      \n",
        "        if x.ndim == 2:          \n",
        "            x = x.unsqueeze(0)\n",
        "        y = aug[\"mask\"].long()    \n",
        "        return x, y\n",
        "\n",
        "def make_loaders(root=\"mri_data\", batch_size=4, val_ratio=0.1, num_workers=4, seed=42):\n",
        "    ids, img_map, mask_map = collect_ids(root)\n",
        "    train_ids, val_ids = split_ids(ids, val_ratio=val_ratio, seed=seed)\n",
        "    train_ds = SpinePNG(train_ids, img_map, mask_map)\n",
        "    val_ds   = SpinePNG(val_ids,   img_map, mask_map)\n",
        "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True,\n",
        "                              num_workers=num_workers, pin_memory=True)\n",
        "    val_loader   = DataLoader(val_ds,   batch_size=batch_size, shuffle=False,\n",
        "                              num_workers=max(1, num_workers//2), pin_memory=True)\n",
        "    return train_loader, val_loader\n",
        "\n",
        "train_loader, val_loader = make_loaders(root=\"/content/mri_data_png/data\", batch_size=4, val_ratio=0.1)\n",
        "\n",
        "for xb, yb in train_loader:\n",
        "    print(xb.shape, yb.shape)  # expect (N,1,256,256) and (N,256,256)\n",
        "    break\n",
        "print(len(train_loader.dataset))  # should be ~231\n",
        "print(len(val_loader.dataset))    # should be ~26"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7UKeUf2gKwLZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "NUM_CLASSES = 20\n",
        "bce = nn.BCEWithLogitsLoss()\n",
        "ce  = nn.CrossEntropyLoss()\n",
        "\n",
        "def dice_loss_from_logits(logits, target_idx, eps=1e-6):\n",
        "    p = F.softmax(logits, dim=1)                           \n",
        "    t = F.one_hot(target_idx, NUM_CLASSES).permute(0,3,1,2).float()\n",
        "    num = 2 * (p * t).sum((0,2,3))\n",
        "    den = (p*p).sum((0,2,3)) + (t*t).sum((0,2,3)) + eps\n",
        "    return 1 - (num/den).mean()\n",
        "\n",
        "def train(gen, disc, train_loader, val_loader, device=\"cuda\", epochs=20, lr=2e-4, lambda_seg=20.0,\n",
        "          save_each_epoch=False):\n",
        "    gen.to(device); disc.to(device)\n",
        "\n",
        "    opt_g = torch.optim.Adam(gen.parameters(),  lr=lr, betas=(0.5, 0.999))\n",
        "    opt_d = torch.optim.Adam(disc.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "    amp_enabled = device.startswith(\"cuda\") and torch.cuda.is_available()\n",
        "    scaler_g = torch.cuda.amp.GradScaler(enabled=amp_enabled)\n",
        "    scaler_d = torch.cuda.amp.GradScaler(enabled=amp_enabled)\n",
        "\n",
        "    for epoch in range(1, epochs+1):\n",
        "        gen.train(); disc.train()\n",
        "        pbar = tqdm(train_loader, desc=f\"epoch {epoch}/{epochs}\", ncols=100, leave=False)\n",
        "\n",
        "        for x, y_idx in pbar:\n",
        "            x = x.to(device, non_blocking=True)\n",
        "            y_idx = y_idx.to(device, non_blocking=True)\n",
        "\n",
        "            with torch.cuda.amp.autocast(enabled=amp_enabled):\n",
        "                with torch.no_grad():\n",
        "                    logits_fake = gen(x)\n",
        "                    probs_fake  = F.softmax(logits_fake, dim=1)  \n",
        "                y_real = F.one_hot(y_idx, NUM_CLASSES).permute(0,3,1,2).float()\n",
        "                d_real = disc(x, y_real)\n",
        "                d_fake = disc(x, probs_fake.detach())\n",
        "                loss_d = bce(d_real, torch.ones_like(d_real)) + bce(d_fake, torch.zeros_like(d_fake))\n",
        "\n",
        "            opt_d.zero_grad(set_to_none=True)\n",
        "            scaler_d.scale(loss_d).backward()\n",
        "            scaler_d.step(opt_d); scaler_d.update()\n",
        "\n",
        "            # ---- G step ----\n",
        "            with torch.cuda.amp.autocast(enabled=amp_enabled):\n",
        "                logits = gen(x)\n",
        "                probs  = F.softmax(logits, dim=1)\n",
        "                d_fake_for_g = disc(x, probs)\n",
        "                gan_loss = bce(d_fake_for_g, torch.ones_like(d_fake_for_g))\n",
        "                seg_loss = ce(logits, y_idx) + dice_loss_from_logits(logits, y_idx)\n",
        "                loss_g = gan_loss + lambda_seg * seg_loss\n",
        "\n",
        "            opt_g.zero_grad(set_to_none=True)\n",
        "            scaler_g.scale(loss_g).backward()\n",
        "            scaler_g.step(opt_g); scaler_g.update()\n",
        "\n",
        "            pbar.set_postfix(D=f\"{loss_d.item():.3f}\",\n",
        "                              G=f\"{loss_g.item():.3f}\",\n",
        "                              CE=f\"{ce(logits, y_idx).item():.3f}\")\n",
        "\n",
        "        gen.eval()\n",
        "        ce_sum, dice_sum, n = 0.0, 0.0, 0\n",
        "        with torch.inference_mode():\n",
        "            for x, y_idx in val_loader:\n",
        "                x = x.to(device); y_idx = y_idx.to(device)\n",
        "                logits = gen(x)\n",
        "                ce_sum   += ce(logits, y_idx).item()\n",
        "                dice_sum += (1.0 - dice_loss_from_logits(logits, y_idx).item())\n",
        "                n += 1\n",
        "\n",
        "        avg_ce   = ce_sum / max(n, 1)\n",
        "        avg_dice = dice_sum / max(n, 1)\n",
        "        print(f\"[epoch {epoch}] val CE: {avg_ce:.3f} | val Dice: {avg_dice:.3f}\")\n",
        "\n",
        "        if save_each_epoch:\n",
        "            torch.save(gen.state_dict(), f\"gen_epoch{epoch}.pth\")\n",
        "\n",
        "    return gen\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hgJKrJJSM_PA",
        "outputId": "b5f4907e-656f-4375-cde3-3488c6bc22a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3493265627.py:25: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler_g = torch.cuda.amp.GradScaler(enabled=amp_enabled)\n",
            "/tmp/ipython-input-3493265627.py:26: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler_d = torch.cuda.amp.GradScaler(enabled=amp_enabled)\n",
            "epoch 1/20:   0%|                                                           | 0/796 [00:00<?, ?it/s]/tmp/ipython-input-3493265627.py:37: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=amp_enabled):\n",
            "/tmp/ipython-input-3493265627.py:51: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(enabled=amp_enabled):\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[epoch 1] val CE: 0.213 | val Dice: 0.468\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[epoch 2] val CE: 0.181 | val Dice: 0.588\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[epoch 3] val CE: 0.134 | val Dice: 0.668\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": []
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[epoch 4] val CE: 0.199 | val Dice: 0.666\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "epoch 5/20:  64%|████████████▊       | 509/796 [10:43<05:58,  1.25s/it, CE=0.056, D=0.044, G=12.732]"
          ]
        }
      ],
      "source": [
        "gen  = Generator(in_ch=1, out_ch=NUM_CLASSES, features=64)\n",
        "disc = Discriminator(in_ch_x=1, in_ch_y=NUM_CLASSES)\n",
        "\n",
        "model = train(gen, disc, train_loader, val_loader, device=\"cpu\", epochs=20, lr=2e-4, lambda_seg=20.0)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V5E1",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
